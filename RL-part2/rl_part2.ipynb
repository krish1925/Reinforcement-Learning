{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy torch wandb swig gymnasium[mujoco] matplotlib termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Part 2: DDPG\n",
    "By Lawrence Liu\n",
    "## Some General Instructions\n",
    "- This entire assigment will be worth 5 points of extra credit for project 4, and will be due on the same day as project 4, so June 7th.\n",
    "- You will be implementing a DDPG agent to solve the DoublePendulum environment.\n",
    "- Because this is a bonus, there will be no test cases.\n",
    "- You will need to implement the TODOs in the `ddpg.py` and `model.py` files.\n",
    "DO NOT use Windows for this project, gymnasium does is not supported for windows and installing it will be difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the Enviroment\n",
    "We will be training a DDPG agent to solve the DoublePendulum environment. The DoublePendulum environment is a classic control problem where the goal is to balance a double pendulum on a cart. \n",
    "#### Action Space\n",
    "The agent can apply a force to the cart in the range of -1 to 1. This is a continuous action space.\n",
    "#### Observation Space\n",
    "The observation space is a 11 dimensional vector. The first 1 is the position of the cart, the next 4 are the cosines and sins of different angles of the double pendulum, and the next 3 are the velocities of the cart and the pendulum, and the final 3 are the constrain forces on the pendulum. You can find more information about these constraint forces [here](https://homes.cs.washington.edu/~todorov/papers/TodorovICRA14.pdf) \n",
    "#### Reward\n",
    "The reward can be decomposed into 3 parts. The first part is an alive bonus that pays +10 for every time step the second pendulum is upright. There are 2 penalty terms, one for the tip of the second pendulum moving too much, and another for the cart moving too fast.\n",
    "\n",
    "You can find more information about the environment [here](https://gymnasium.farama.org/environments/mujoco/inverted_double_pendulum/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "env = gym.make(\"InvertedDoublePendulum-v4\")\n",
    "env.np_random = np.random.RandomState(42)\n",
    "\n",
    "eval_env = gym.make(\"InvertedDoublePendulum-v4\", render_mode=\"rgb_array\")\n",
    "eval_env.np_random = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "frames = []\n",
    "s, _ = eval_env.reset()\n",
    "\n",
    "while True:\n",
    "    a = eval_env.action_space.sample()\n",
    "    s, r, terminated, truncated, _ = eval_env.step(a)\n",
    "    frames.append(eval_env.render())\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "\n",
    "anim = animate(frames)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (1 point)\n",
    "Because the inputs to the model is a 11 dimensional vector, we will use a MLP. Specifically we will follow the architecture in the DDPG paper. For DDPG we have both an Actor and a Critic. The Actor is responsible for selecting the action, and the Critic is responsible for evaluating the action. \n",
    "#### Actor\n",
    "The Actor is a 3 layer MLP:\n",
    "- Layer 1: 400 units, ReLU activation, Fan-in weight initialization, ie each weight is initialized with a uniform distribution in the range of -1/sqrt(fan_in) to 1/sqrt(fan_in)\n",
    "- Layer 2: 300 units, ReLU activation, Fan-in weight initialization, ie each weight is initialized with a uniform distribution in the range of -1/sqrt(fan_in) to 1/sqrt(fan_in)\n",
    "- Layer 3: 1 unit, tanh activation, intialized with uniform weights in the range of -0.003 to 0.003\n",
    "#### Critic\n",
    "The Critic is a 3 layer MLP:\n",
    "- Layer 1: 400 units, ReLU activation, Fan-in weight initialization, ie each weight is initialized with a uniform distribution in the range of -1/sqrt(fan_in) to 1/sqrt(fan_in)\n",
    "- Layer 2: 300 units, ReLU activation, Fan-in weight initialization, ie each weight is initialized with a uniform distribution in the range of -1/sqrt(fan_in) to 1/sqrt(fan_in). Input is the concatenation of the 400 dimension embedding from the state, and the action taken.\n",
    "- Layer 3: 1 unit, intialized with uniform weights in the range of -0.003 to 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration (1 point)\n",
    "Because DDPG is an off policy algorithm, we will use a noise process to encourage exploration. Specifically we will use the Ornstein-Uhlenbeck process. The Ornstein-Uhlenbeck process is a stochastic process that generates temporally correlated noise. The process is defined by the following stochastic differential equation:\n",
    "$$dx_t = \\theta(\\mu - x_t)dt + \\sigma dW_t$$\n",
    "Where $\\theta$ is the rate of mean reversion, $\\mu$ is the long run mean of the process, $\\sigma$ is the volatility of the process, and $W_t$ is a Wiener process. We can discretize this process to get the following:\n",
    "$$x_{t+1} = x_t + \\theta(\\mu - x_t)dt + \\sigma \\sqrt{dt}\\mathcal{N}(0,1)$$\n",
    "Where $N(0,1)$ is a sample from the standard normal distribution. We will asume that our steps are of unit length, so we can simplify this to:\n",
    "$$x_{t+1} = x_t + \\theta(\\mu - x_t) + \\sigma \\mathcal{N}(0,1)$$\n",
    "We will use $\\theta = 0.15$, $\\mu = 0$, and $\\sigma = 0.2$. We will add this to our action in the following way\n",
    "$$a_t = \\min(\\max(\\mu(s_t) + x_t, -1), 1)$$\n",
    "Where $a_t$ is the action taken by the agent, $\\mu(s_t)$ is the action selected by the actor, and $x_t$ is the noise generated by the Ornstein-Uhlenbeck process.\n",
    "Please implement the `OU_Noise` class in DDPG.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG (3 points total)\n",
    "We will be implementing the DDPG algorithm. The DDPG algorithm is a model free, off policy algorithm that combines the actor-critic architecture with the insights of DQN. The algorithm is as follows:\n",
    "![DDPG](DDPG.png)\n",
    "Fill in the TODOs in the `DDPG` class in DDPG.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DDPG\n",
    "import utils\n",
    "t = DDPG.DDPG(env,\n",
    "            model.Actor,\n",
    "            model.Critic,\n",
    "            use_wandb=True,\n",
    "            save_path = utils.get_save_path(\"DDPG\",\"./runs/\"))\n",
    "\n",
    "t.train(10000,\n",
    "        100,\n",
    "        100,\n",
    "        1000,\n",
    "        100,\n",
    "        1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like what we did for the DQN, we can also animate one episode of the agent in the DoublePendulum environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards, frames = t.play_episode(0,True,42,eval_env)\n",
    "anim = animate(frames,max_frames = 1000)\n",
    "print(total_rewards)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the agent is able to balance the double pendulum and it eventually reaches the equilibrium. However this equilibrium is not a stable equilibrium, so lets see how this model performs with perturbations. To do this, we will perturbe the model every 49 steps with a large input of $\\pm 0.75$ N to the cart. We will see how the model performs with this perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "frames = []\n",
    "scores = 0\n",
    "(s, _), done, ret = eval_env.reset(seed = 42\n",
    "                                   ), False, 0\n",
    "t.actor.eval()\n",
    "S = []\n",
    "outputs = []\n",
    "# s, r, terminated, truncated, info = eval_env.step(3)\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    while not done:\n",
    "        # if random.random() < 0.1:\n",
    "        #     action = random.randint(0,4)\n",
    "        # else:\n",
    "        frames.append(eval_env.render())\n",
    "        output = t.actor(torch.tensor(s).unsqueeze(0).to(\"cpu\").float())\n",
    "        i+=1\n",
    "        if i%50 == 49:\n",
    "            output += 0.75*(np.sign(torch.randn_like(output)))\n",
    "        s_prime, r, terminated, truncated, info = eval_env.step(output.cpu().numpy().squeeze(0))\n",
    "        s = s_prime\n",
    "        ret += r\n",
    "        done = terminated or truncated\n",
    "        \n",
    "scores += ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animate(frames,max_frames = 500)\n",
    "print(total_rewards)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the model is able to recover from the perturbation and is able to balance the double pendulum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
